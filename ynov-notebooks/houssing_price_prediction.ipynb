{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "\n",
      "Colonnes dans les données:\n",
      "Train: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity', 'id']\n",
      "Valid: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity', 'id', 'prediction']\n",
      "Test: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'ocean_proximity', 'id']\n",
      "\n",
      "Valeurs manquantes dans X_train: 102\n",
      "Valeurs manquantes dans X_valid: 21\n",
      "Valeurs manquantes dans X_test: 84\n",
      "\n",
      "Génération des features d'ensemble...\n",
      "Training model 1/5...\n",
      "Training model 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.895e+12, tolerance: 1.018e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.909e+12, tolerance: 1.011e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.851e+12, tolerance: 1.016e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+12, tolerance: 1.031e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.973e+12, tolerance: 1.039e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+12, tolerance: 1.279e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.780e+13, tolerance: 1.018e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+13, tolerance: 1.011e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.731e+13, tolerance: 1.016e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.789e+13, tolerance: 1.031e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+13, tolerance: 1.039e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\ouchaou\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ynov-ix4t_1a0-py3.13\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.211e+13, tolerance: 1.279e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 4/5...\n",
      "Training model 5/5...\n",
      "Combinaison des features...\n",
      "Entraînement du modèle final...\n",
      "\n",
      "Performances sur l'ensemble de validation:\n",
      "R² score: 0.8105\n",
      "RMSE: 50956.15\n",
      "\n",
      "Génération des prédictions de test...\n",
      "Création du fichier de soumission...\n",
      "\n",
      "Terminé! Fichier de soumission créé avec succès.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class EnsembleFeatureGenerator:\n",
    "    \"\"\"\n",
    "    Classe pour générer des features d'ensemble en utilisant plusieurs modèles\n",
    "    Implémente une approche de stacking pour améliorer les prédictions\n",
    "    \"\"\"\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        self.preprocessors = []\n",
    "        \n",
    "    def fit_transform(self, X_train, y_train, X_valid):\n",
    "        # Configuration de la validation croisée avec 5 plis\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Initialisation des arrays pour stocker les prédictions\n",
    "        train_predictions = np.zeros((X_train.shape[0], len(self.models)))\n",
    "        valid_predictions = np.zeros((X_valid.shape[0], len(self.models)))\n",
    "        \n",
    "        # Identification des features catégorielles et numériques\n",
    "        categorical_features = [col for col in ['ocean_proximity'] if col in X_train.columns]\n",
    "        numeric_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "        \n",
    "        # Boucle sur chaque modèle\n",
    "        for i, model in enumerate(self.models):\n",
    "            print(f\"Training model {i+1}/{len(self.models)}...\")\n",
    "            \n",
    "            # Pipeline de prétraitement pour les variables numériques\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),  # Gestion des valeurs manquantes\n",
    "                ('scaler', StandardScaler())  # Standardisation des données\n",
    "            ])\n",
    "            \n",
    "            # Pipeline de prétraitement pour les variables catégorielles\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Gestion des valeurs manquantes\n",
    "                ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))  # Encodage one-hot\n",
    "            ])\n",
    "            \n",
    "            # Créer le préprocesseur avec gestion des valeurs manquantes\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numeric_transformer, numeric_features),\n",
    "                    ('cat', categorical_transformer, categorical_features)\n",
    "                ] if categorical_features else [\n",
    "                    ('num', numeric_transformer, numeric_features)\n",
    "                ])\n",
    "            \n",
    "            # Prédictions de validation croisée\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "                X_fold_train = X_train.iloc[train_idx].copy()\n",
    "                y_fold_train = y_train.iloc[train_idx].copy()\n",
    "                X_fold_val = X_train.iloc[val_idx].copy()\n",
    "                \n",
    "                X_fold_train_processed = preprocessor.fit_transform(X_fold_train)\n",
    "                X_fold_val_processed = preprocessor.transform(X_fold_val)\n",
    "                \n",
    "                model.fit(X_fold_train_processed, y_fold_train)\n",
    "                train_predictions[val_idx, i] = model.predict(X_fold_val_processed)\n",
    "            \n",
    "            # Entraînement final\n",
    "            X_train_processed = preprocessor.fit_transform(X_train)\n",
    "            X_valid_processed = preprocessor.transform(X_valid)\n",
    "            \n",
    "            model.fit(X_train_processed, y_train)\n",
    "            valid_predictions[:, i] = model.predict(X_valid_processed)\n",
    "            self.preprocessors.append(preprocessor)\n",
    "        \n",
    "        return train_predictions, valid_predictions, X_train_processed, X_valid_processed\n",
    "    \n",
    "    def transform_test(self, X_test):\n",
    "        test_predictions = np.zeros((X_test.shape[0], len(self.models)))\n",
    "        X_test_processed = None\n",
    "        \n",
    "        for i, (model, preprocessor) in enumerate(zip(self.models, self.preprocessors)):\n",
    "            if i == 0:  # Garder le premier preprocessing pour les features originales\n",
    "                X_test_processed = preprocessor.transform(X_test)\n",
    "            test_processed = preprocessor.transform(X_test)\n",
    "            test_predictions[:, i] = model.predict(test_processed)\n",
    "            \n",
    "        return test_predictions, X_test_processed\n",
    "\n",
    "# Chargement des données\n",
    "print(\"Chargement des données...\")\n",
    "train_data = pd.read_csv('./ynov-data/train_housing_train.csv')\n",
    "valid_data = pd.read_csv('./ynov-data/train_housing_valid.csv')\n",
    "test_data = pd.read_csv('./ynov-data/test_housing.csv')\n",
    "\n",
    "# Vérification des colonnes\n",
    "print(\"\\nColonnes dans les données:\")\n",
    "print(\"Train:\", train_data.columns.tolist())\n",
    "print(\"Valid:\", valid_data.columns.tolist())\n",
    "print(\"Test:\", test_data.columns.tolist())\n",
    "\n",
    "# Séparer features et target\n",
    "X_train = train_data.drop(['median_house_value', 'id'], axis=1)\n",
    "y_train = train_data['median_house_value']\n",
    "X_valid = valid_data.drop(['median_house_value', 'id', 'prediction'], axis=1)\n",
    "y_valid = valid_data['median_house_value']\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# Vérification des valeurs manquantes\n",
    "print(\"\\nValeurs manquantes dans X_train:\", X_train.isnull().sum().sum())\n",
    "print(\"Valeurs manquantes dans X_valid:\", X_valid.isnull().sum().sum())\n",
    "print(\"Valeurs manquantes dans X_test:\", X_test.isnull().sum().sum())\n",
    "\n",
    "# Définir les modèles avec des paramètres plus robustes\n",
    "models = [\n",
    "    # Modèles linéaires régularisés\n",
    "    Ridge(alpha=1.0, random_state=42),  # Régularisation L2\n",
    "    Lasso(alpha=0.001, random_state=42),  # Régularisation L1\n",
    "    ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=42),  # Combinaison L1 et L2\n",
    "    # Modèles ensemblistes\n",
    "    RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),  # Forêt aléatoire\n",
    "    GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)  # Gradient Boosting\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Phase 1: Génération des features d'ensemble\n",
    "    print(\"\\nGénération des features d'ensemble...\")\n",
    "    ensemble_generator = EnsembleFeatureGenerator(models)\n",
    "    train_meta_features, valid_meta_features, X_train_processed, X_valid_processed = ensemble_generator.fit_transform(\n",
    "        X_train, y_train, X_valid\n",
    "    )\n",
    "\n",
    "    # Phase 2: Combinaison des features originales et des meta-features\n",
    "    print(\"Combinaison des features...\")\n",
    "    X_train_enhanced = np.hstack([X_train_processed, train_meta_features])\n",
    "    X_valid_enhanced = np.hstack([X_valid_processed, valid_meta_features])\n",
    "\n",
    "    # Phase 3: Entraînement du modèle final\n",
    "    print(\"Entraînement du modèle final...\")\n",
    "    final_model = LinearRegression()\n",
    "    final_model.fit(X_train_enhanced, y_train)\n",
    "\n",
    "    # Phase 4: Évaluation du modèle\n",
    "    valid_predictions = final_model.predict(X_valid_enhanced)\n",
    "    valid_predictions = np.clip(valid_predictions, 0, None)  # Clip des valeurs négatives\n",
    "    r2 = r2_score(y_valid, valid_predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, valid_predictions))\n",
    "\n",
    "    print(f\"\\nPerformances sur l'ensemble de validation:\")\n",
    "    print(f\"R² score: {r2:.4f}\")  # Coefficient de détermination\n",
    "    print(f\"RMSE: {rmse:.2f}\")    # Erreur quadratique moyenne\n",
    "\n",
    "    # Phase 5: Génération des prédictions finales\n",
    "    print(\"\\nGénération des prédictions de test...\")\n",
    "    test_meta_features, X_test_processed = ensemble_generator.transform_test(X_test)\n",
    "    X_test_enhanced = np.hstack([X_test_processed, test_meta_features])\n",
    "    test_predictions = final_model.predict(X_test_enhanced)\n",
    "    test_predictions = np.clip(test_predictions, 0, None)\n",
    "\n",
    "    # Phase 6: Création du fichier de soumission\n",
    "    print(\"Création du fichier de soumission...\")\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_data['id'], \n",
    "        'median_house_value': test_predictions\n",
    "    })\n",
    "    submission.to_csv('./ynov-data/submit.csv', index=False)\n",
    "    print(\"\\nTerminé! Fichier de soumission créé avec succès.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nUne erreur s'est produite: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
